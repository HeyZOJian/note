[TOC]

# 第一章

## 一、一些概念

### 1. 监督学习的数据

分为**训练集（75%）**和**测试集（25%）**

前者用于构建模型，后者用于评估模型对前所未见的新数据的泛化能力

### 2. 样本，特征，形状

样本：机器学习中的个体

特征：个体的属性

形状：样本数 * 特征数

## 二、构建机器学习模型的步骤

1. 观察数据
   1. 目的
      1. 看看如果不用机器学习能不能轻松完成任务，或者需要的信息有没有包含在数据中
      2. 发现异常值和特殊值
      3. 检查数据最佳的方法之一是将其可视化（散点图，散点图矩阵【无法显示三维及以上的特征关系】）
2. 构建模型
   1. 寻找合适的算法
   2. 输入训练数据进行训练
3. 做出预测
4. 评估模型
   1. 利用测试数据进行精度评估



# 第二章 监督学习

## 一、问题的分类

### 1. 分类问题 classification

#### 目标

预测类别标签（class label）,这些标签来自预定义的可选列表

#### 类别

1. 二分类
   1. 正类：研究对象（如垃圾邮件分类中的垃圾邮件）
   2. 反类
2. 多分类



### 2. 回归问题 regression

#### 目标

预测一个连续值，编程术语叫**浮点数**，数学术语叫**实数**



## 二、泛化、过拟合与欠拟合

### 1. 泛化（generalize）

模型能对没见过的数据做出准确预测，我们就说他能够从训练集**泛化**到测试集，我们的目标就是要构建一个泛化精度尽可能高的模型

### 2. 过拟合（overfitting）

模型过于复杂，过分关注训练集的细节，导致在训练集表现很好，但是不能泛化到新数据上

### 3. 欠拟合（underfitting）

模型过于简单，无法抓住数据的全部内容以及数据中的变化，在训练集上表现很差

![模型复杂度与训练精度和测试精度之间的权衡](/Users/zhuangzhongjian/Desktop/006tNbRwgy1fu8jrpf26yj30jo0ea40z.jpg)

### 4. 交互项

特征之间的乘积

### 5. 特征工程（feature engineering）

包含导出特征（交互项）的方法



## 三、K近邻

### 1. K近邻分类

#### 思路

想要对新数据点进行预测，算法会在训练数据集中找到最近的数据点，也就是他的“最近邻”来进行判断。

除了考虑最近邻，还可以考虑任意个（k个）邻居。在考虑多个邻居的时候，我们用**投票法**来指定。

#### 模型复杂度

随着邻居个数的增加，决策边界（decision boundary）就越平滑，模型也就越简单。

| 邻居个数 | 模型复杂度 | 训练集精度 | 测试集精度 |
| :------: | :--------: | :--------: | :--------: |
|    少    |    复杂    |     高     |     低     |
|    多    |    简单    |     低     |     高     |

![模型复杂度与训练精度和测试精度之间的权衡](https://ws1.sinaimg.cn/large/006tNbRwgy1fu8jsfb5jej30hv0diq3l.jpg)

### 2. K近邻回归

#### 思路

在使用多个近邻时，预测结果为这些邻居的**平均值**

#### R^2分数（决定系数）

用score方法来评估模型，对于回归问题，返回R^2分数。

1：表示完美预测，

0：表示常数模型，即总是预测训练集响应（y_train）的平均值。



### 3. 优缺点

#### 优点

1. 模型容易理解
2. 不需过多调节就可以得到不错的性能
3. 构建模型的速度快

#### 缺点

1. 不能处理多特征的数据集
2. 预测速度慢
3. 对稀疏数据集效果不好



## 四、线性模型

### 1. 用于回归的线性模型

1. 一般公式：`ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b  `
   1.  x[0] 到 x[p] 表示单个数据点的特征 
   2. w 和 b 是学习模型的参数
   3. ŷ 是模型的预测结果
2. 对于单一特征的数据集，公式：`ŷ = w[0] * x[0] + b `
   1. 可以看成直线方程，w为斜率，b与截距
   2. 可以看成输入特征的加权求和，权重由w的元素给出
3. 不同的线性回归模型之间的区别在于：
   1. 如何从训练数据中学习参数w和b
   2. 如何控制模型复杂度

#### 线性回归（普通最小二乘法）

1. 目标：寻址参数w和b，使得对训练集的预测值与真实的回归目标值y之间的**均方误差**最小。（均方误差：预测值与真实值之差的平方和除以样本数）
2. 一维数据集情况下容易出现**欠拟合**：训练集和测试集的分数非常接近
3. 多维数据集情况下容易出现**过拟合**：训练集的预测非常准确，但是测试集的R^2的分数非常低

#### 岭回归（ridge regression ）

> 一个可以控制复杂度的模型，约束更强，不容易出现过拟合

1. 特点

   1. 公式与**普通最小二乘法**一样
   2. 拟合附加约束
   3. 系数尽量小，即w尽量接近0，即每个特征对输出的影响应尽可能小 

2. 正则化（regularization ）

   1. 特点中的2，3的约束就是正则化
   2. 岭回归用到的正则化是 **L2正则化**

3. alpha 参数

   1. 设置简单性和训练 集性能二者对于模型的重要程度 

   2. ```python
      ridge10 = Ridge(alpha=10).fit(X_train, y_train)
      ```

   3. alpha 的最佳设定 值取决于用到的具体数据集 

   4. 增大 alpha 会使得系数更加趋向于 0，从而降低训练集性能， 但可能会提高泛化性能 

#### 学习曲线

数据集大小和模型性能的图像

#### lasso 

1. 与岭回归相同，但是使用**L1正则化**（某些系数刚好为 0。这说明某些特征被模型完全忽略。这可以看作是一种自动化的特征选 择。 ）
2. 参数
   1. alpha：控制系数趋向于0的强度
   2. max_iter：运行迭代的最大次数



### 2. 用于分类的线性函数

#### 二分类

1. 公式：`ŷ = w[0] * x[0] + w[1] * x[1] + ...+ w[p] * x[p] + b > 0 `
   1. 看起来和线性回归的公式非常相似，但我们没有返回特征的加权求和，而是为预 测设置了**阈值(0)**
   2. 小于0：返回预测类别-1
   3. 大于0：返回预测类别+1 







### 3. 朴素贝叶斯分类器

训练速度快，泛化能力稍差

1. **高效的原因**：单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据。

2. 三种贝叶斯分类器的实现：

   1. 高斯贝叶斯GaussianNB：应用于连续数据，主要用于高维数据
   2. 伯努利贝叶斯BernoulliNB：假定输入数据为二分类数据，和多项式贝叶斯都广泛用于稀疏计数数据
   3. 多项式贝叶斯MultinomialNB：假定输入数据为计数数据（即每个特征代表某个对象的整数计数，比如一个单词在文章中的出现次数）

3. 贝叶斯定理

   1. 正向概率：比如已知三个人的偷盗成功概率，求村子被盗的概率？

   2. 逆向概率：比如已知村子被盗和这三个人的偷盗成功概率，求这三个人与案件有关的概率？

   3. 事件A在事件B（发生）的条件下的概率，与事件B在事件A（发生）的条件下的概率是不一样的，但它们两者之间是有确定的关系的，贝叶斯定理陈述了这个关系。

   4. 公式：
      $$
      P(A|B) = \frac{P(B|A)P(A)}{P(B)}
      $$

4. **朴素贝叶斯的概率模型推导：**

   1. X 待分类项

   2. fi是X的一个特征属性 i∈[1,n]

   3. Ci是一个类别 i∈[1,m]

   4. 计算
      $$
      P(C_k|X) = MAX(P(C_1|X), P(C_2|X),...,P(C_m|X))
      $$
      则X∈Ck，即取概率最大的那个

      1. 统计在各类别下各特征属性的条件概率P(f1|C1)，P(f2|C1),..,P(f1|C2),...,P(fn|Cm)

      2. 根据贝叶斯定理，推导得
         $$
         P(C_i|X)=\frac{P(X|C_i)P(C_i)}{P(X)}
         $$
         由于P(X)是个常数，所以只要分子最大化即可，即推得
         $$
         P(X|C_i)P(C_i) = P(f_1|C_i)P(f_2|C_i),...,P(f_n|C_i)P(C_i) = P(C_i)\prod_{j=1}^n{P(f_j|C_i)}
         $$
         

### 4. 决策树

### 5.决策树集成

1. **集成**：
   1. 定义：合并多个机器学习模型来构建更强大的模型的方法
   2. 两种有效的集成模型：
      1. 随机森林
      2. 梯度提示决策树
2. **随机森林**：
   1. 本质：许多决策树的集合，每棵树都和其他数略有不同
   2. 目的：解决决策树经常对训练数据过拟合的情况
   3. 思想：每棵树的预测都很好，但是以不同的方式过拟合，那么我们可以对这些树的结果取平均值来降低过拟合，这样既能减少过拟合又能保证树的预测能力。
   4. 构造随机森林：
      1. 确定构造的树的个数
      2. 每构造一棵树前对数据**自助采样（bootstrap sample）**，即从n个数据点中有放回地重复随机抽取一个样本，共抽取n次。这样会创建一个与原数据集大小相同的数据集，但有些数据点会缺少或重复。
      3. 基于新创建的数据集来构造决策树
         1. 在每个结点处，算法随机选择特征的一个子集，并对其中的一个特征寻址最佳测试，而不是对每个结点都寻址最佳测试。选择的特征个数由`max_features`参数决定。每个结点中特征子集的选择是相互独立，这样树的每个结点可以使用特征的不同子集来做出决策。
         2. `max_features`等于`n_features`时，每次划分就都要考虑所有特征，就没有了随机性（不过自助采样依然存在随机性）
         3. `max_features`等于1时，那么在划分时将无法选择对哪个特征进行测试，只能对随机选择的某个特征搜索不同的阀值。
      4. 对于回归问题：对每棵树的结果取平均值
      5. 对于分类问题：利用**软投票**，即每棵树给出每个可能的输出标签的概率，然后对所有树的预测概率取平均值，将概率最大的类别作为预测结果
   5. 优点：不需要返回调参就可以给出很好的结果，也不需要对数据进行缩放
   6. 缺点：做详细解释时可视化差
   7. 参数：利用`n_jobs`设置使用的内核个数，-1时使用计算机的所有内核